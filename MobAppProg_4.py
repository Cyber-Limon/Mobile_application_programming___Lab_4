import random
import numpy as np
import matplotlib.pyplot as plt


dataset = [
    ([[7.5, 209, 13.2, 3.18, 4, 20, 2.4, 1, 1, 4.2]], 0),
    ([[8.1, 193, 14.0, 2.96, 4, 14, 3.0, 2, 1, 3.9]], 0),
    ([[8.4, 188, 15.4, 3.91, 2, 21, 3.2, 2, 1, 4.9]], 0),
    ([[7.9, 221, 15.1, 3.1, 1, 15, 2.1, 2, 1, 4.7]], 0),
    ([[7.7, 241, 14.7, 3.17, 2, 22, 3.0, 1, 1, 4.6]], 0),
    ([[7.2, 228, 15.2, 3.44, 1, 19, 3.2, 1, 1, 3.7]], 0),
    ([[8.3, 187, 13.6, 3.68, 2, 23, 3.4, 2, 1, 3.9]], 0),
    ([[8.5, 221, 14.2, 3.92, 4, 20, 2.4, 1, 1, 4.3]], 0),
    ([[7.8, 179, 15.0, 3.93, 4, 23, 2.3, 2, 1, 4.7]], 0),
    ([[7.8, 201, 15.4, 3.19, 2, 23, 2.6, 2, 1, 4.1]], 0),
    ([[7.2, 196, 14.8, 3.23, 4, 17, 3.0, 1, 1, 5.0]], 0),
    ([[8.4, 258, 14.0, 3.88, 3, 17, 2.4, 2, 1, 4.5]], 0),
    ([[9.1, 204, 15.3, 3.55, 3, 21, 2.1, 1, 1, 4.9]], 0),
    ([[9.2, 205, 14.9, 3.4, 4, 18, 2.3, 1, 1, 4.4]], 0),
    ([[8.9, 174, 13.0, 3.08, 4, 19, 2.7, 2, 1, 3.9]], 0),
    ([[9.3, 204, 14.4, 3.87, 4, 22, 2.7, 2, 1, 4.3]], 0),
    ([[7.3, 243, 15.7, 3.45, 3, 18, 2.7, 2, 1, 4.4]], 0),
    ([[8.8, 181, 14.4, 3.79, 2, 23, 2.3, 1, 1, 3.6]], 0),
    ([[9.2, 228, 14.5, 3.4, 4, 23, 2.4, 2, 1, 4.1]], 0),
    ([[9.2, 220, 13.4, 3.23, 3, 16, 2.4, 1, 1, 4.6]], 0),
    ([[8.6, 240, 14.6, 3.8, 3, 22, 2.7, 1, 1, 3.7]], 0),
    ([[8.7, 199, 15.9, 3.03, 4, 18, 2.8, 2, 1, 4.7]], 0),
    ([[8.7, 246, 13.3, 3.08, 4, 22, 2.6, 2, 1, 4.3]], 0),
    ([[7.9, 190, 13.3, 3.18, 4, 16, 2.5, 1, 1, 4.5]], 0),
    ([[7.3, 201, 15.6, 3.43, 1, 23, 2.7, 2, 1, 4.5]], 0),
    ([[8.3, 235, 16.0, 3.12, 1, 23, 3.4, 2, 1, 3.8]], 0),
    ([[9.4, 260, 14.1, 2.89, 1, 20, 3.1, 2, 1, 4.2]], 0),
    ([[7.3, 220, 14.3, 3.14, 4, 15, 2.4, 1, 1, 3.8]], 0),
    ([[7.4, 252, 13.2, 3.93, 3, 18, 3.5, 2, 1, 3.7]], 0),
    ([[8.3, 258, 15.7, 3.92, 3, 19, 2.5, 1, 1, 4.0]], 0),
    ([[8.9, 174, 9.5, 4.12, 4, 10, 3.3, 4, 1, 2.9]], 1),
    ([[6.3, 239, 12.5, 3.98, 5, 13, 1.3, 4, 1, 2.6]], 1),
    ([[6.7, 215, 12.2, 4.01, 5, 9, 3.2, 4, 1, 2.6]], 1),
    ([[9.0, 286, 7.4, 3.96, 5, 11, 3.7, 2, 1, 3.1]], 1),
    ([[8.4, 253, 12.5, 4.32, 3, 12, 1.5, 4, 1, 2.7]], 1),
    ([[7.9, 178, 6.2, 3.88, 5, 16, 4.1, 2, 1, 2.7]], 1),
    ([[8.5, 220, 6.9, 3.87, 4, 13, 2.8, 2, 1, 2.9]], 1),
    ([[7.9, 228, 6.1, 3.27, 4, 7, 2.5, 3, 1, 3.1]], 1),
    ([[7.2, 236, 10.8, 3.19, 4, 8, 2.7, 2, 1, 2.7]], 1),
    ([[9.8, 141, 7.9, 3.57, 4, 9, 1.9, 4, 1, 2.6]], 1),
    ([[8.0, 135, 8.2, 4.04, 3, 15, 2.8, 4, 1, 3.1]], 1),
    ([[7.0, 157, 6.7, 4.07, 3, 16, 3.8, 3, 1, 3.0]], 1),
    ([[8.0, 183, 8.2, 3.41, 4, 14, 4.0, 3, 1, 2.9]], 1),
    ([[6.5, 124, 7.6, 3.85, 5, 14, 1.1, 3, 1, 2.9]], 1),
    ([[6.9, 113, 12.5, 3.93, 3, 16, 4.9, 2, 1, 2.8]], 1),
    ([[6.3, 262, 12.6, 3.53, 3, 11, 3.7, 2, 1, 3.2]], 1),
    ([[7.9, 263, 9.4, 3.39, 5, 9, 1.4, 4, 1, 3.5]], 1),
    ([[7.7, 285, 11.3, 4.18, 5, 10, 3.8, 3, 1, 3.2]], 1),
    ([[7.9, 238, 11.4, 3.28, 4, 8, 4.4, 2, 1, 3.4]], 1),
    ([[6.7, 145, 11.6, 3.18, 4, 12, 1.0, 3, 1, 3.2]], 1),
    ([[7.1, 257, 9.6, 3.64, 5, 13, 1.3, 3, 1, 3.3]], 1),
    ([[9.8, 250, 12.1, 3.65, 5, 9, 2.5, 4, 1, 3.0]], 1),
    ([[8.1, 291, 6.9, 3.85, 4, 8, 3.1, 3, 1, 2.9]], 1),
    ([[9.3, 195, 12.2, 4.01, 5, 9, 3.9, 2, 1, 2.8]], 1),
    ([[9.0, 180, 8.6, 3.51, 3, 8, 4.2, 3, 1, 2.9]], 1),
    ([[8.0, 243, 6.7, 3.54, 5, 9, 4.0, 3, 1, 3.3]], 1),
    ([[9.8, 189, 6.7, 4.38, 3, 16, 4.6, 2, 1, 2.6]], 1),
    ([[8.0, 127, 9.9, 3.38, 5, 15, 2.0, 3, 1, 3.3]], 1),
    ([[6.2, 239, 11.1, 3.84, 4, 14, 4.3, 4, 1, 3.1]], 1),
    ([[7.9, 218, 10.5, 3.39, 3, 11, 4.0, 2, 0, 4.5]], 2),
    ([[7.9, 216, 11.5, 3.24, 3, 13, 4.4, 4, 0, 3.5]], 2),
    ([[8.3, 199, 10.9, 3.22, 3, 11, 4.3, 2, 0, 4.3]], 2),
    ([[8.0, 197, 10.8, 3.38, 3, 14, 4.1, 2, 0, 4.5]], 2),
    ([[7.8, 206, 11.2, 3.33, 3, 11, 4.3, 2, 0, 4.5]], 2),
    ([[8.0, 202, 10.8, 3.36, 3, 15, 4.1, 2, 0, 3.6]], 2),
    ([[8.0, 191, 10.9, 3.26, 3, 12, 4.1, 2, 0, 3.9]], 2),
    ([[8.0, 208, 10.5, 3.25, 3, 15, 4.4, 2, 0, 3.7]], 2),
    ([[8.3, 192, 10.7, 3.33, 3, 13, 4.2, 2, 0, 3.8]], 2),
    ([[8.3, 195, 11.4, 3.24, 3, 12, 4.5, 2, 0, 4.4]], 2),
    ([[8.1, 217, 10.7, 3.34, 3, 13, 4.4, 2, 0, 4.1]], 2),
    ([[8.0, 206, 11.0, 3.32, 3, 14, 4.5, 2, 0, 4.2]], 2),
    ([[8.0, 216, 10.7, 3.29, 3, 11, 4.0, 2, 0, 4.3]], 2),
    ([[8.0, 196, 11.4, 3.21, 3, 12, 4.5, 2, 0, 3.9]], 2),
    ([[7.9, 193, 11.4, 3.37, 3, 12, 4.3, 2, 0, 4.3]], 2),
    ([[7.8, 190, 11.4, 3.3, 3, 12, 4.3, 4, 0, 3.6]], 2),
    ([[8.2, 218, 11.4, 3.22, 3, 12, 4.4, 2, 0, 4.0]], 2),
    ([[7.9, 203, 10.7, 3.3, 3, 13, 4.3, 2, 0, 4.1]], 2),
    ([[7.9, 199, 11.4, 3.24, 3, 12, 4.3, 4, 0, 3.7]], 2),
    ([[7.8, 204, 10.9, 3.31, 3, 13, 4.4, 4, 0, 4.0]], 2),
    ([[7.8, 205, 11.2, 3.21, 3, 12, 4.1, 2, 0, 3.8]], 2),
    ([[8.2, 195, 10.8, 3.35, 3, 14, 4.3, 4, 0, 3.8]], 2),
    ([[8.1, 215, 10.7, 3.2, 3, 13, 4.0, 2, 0, 3.8]], 2),
    ([[7.9, 204, 10.6, 3.21, 3, 12, 4.4, 2, 0, 4.2]], 2),
    ([[8.3, 196, 11.1, 3.27, 3, 14, 4.4, 4, 0, 3.6]], 2),
    ([[7.9, 218, 11.0, 3.24, 3, 12, 4.2, 4, 0, 3.9]], 2),
    ([[8.3, 194, 10.9, 3.31, 3, 13, 4.0, 4, 0, 4.2]], 2),
    ([[7.8, 216, 10.9, 3.34, 3, 14, 4.2, 2, 0, 3.5]], 2),
    ([[8.2, 197, 11.3, 3.4, 3, 11, 4.2, 2, 0, 4.1]], 2),
    ([[8.2, 215, 10.6, 3.33, 3, 14, 4.2, 4, 0, 4.5]], 2),
    ([[9.1, 290, 7.9, 4.02, 4, 25, 3.4, 3, 0, 3.4]], 3),
    ([[9.2, 273, 7.5, 4.0, 4, 21, 3.2, 3, 0, 3.3]], 3),
    ([[9.5, 290, 7.6, 4.02, 5, 21, 3.4, 3, 0, 3.4]], 3),
    ([[9.8, 271, 7.2, 4.05, 4, 24, 3.3, 3, 0, 3.1]], 3),
    ([[9.1, 291, 7.9, 3.95, 4, 23, 3.3, 3, 0, 3.1]], 3),
    ([[9.8, 261, 8.5, 3.95, 4, 23, 3.3, 3, 0, 3.2]], 3),
    ([[9.3, 271, 7.6, 3.91, 4, 25, 3.1, 3, 0, 3.5]], 3),
    ([[9.2, 276, 7.3, 3.89, 5, 21, 3.1, 3, 0, 3.2]], 3),
    ([[9.6, 264, 7.6, 3.92, 4, 25, 3.0, 3, 0, 3.3]], 3),
    ([[9.5, 290, 7.3, 3.98, 5, 24, 3.4, 3, 0, 3.0]], 3),
    ([[9.4, 271, 8.3, 3.94, 5, 22, 3.4, 3, 0, 3.3]], 3),
    ([[9.3, 269, 7.1, 3.88, 4, 21, 3.4, 3, 0, 3.4]], 3),
    ([[9.7, 282, 7.0, 3.94, 5, 21, 3.2, 3, 0, 3.3]], 3),
    ([[9.2, 277, 7.8, 4.06, 5, 23, 3.1, 3, 0, 3.0]], 3),
    ([[9.1, 267, 8.0, 3.82, 4, 25, 3.1, 3, 0, 3.4]], 3),
    ([[9.6, 263, 7.2, 3.94, 5, 23, 3.2, 3, 0, 3.4]], 3),
    ([[9.8, 258, 7.4, 3.95, 5, 22, 3.0, 3, 0, 3.4]], 3),
    ([[9.4, 299, 8.4, 3.94, 5, 22, 3.4, 3, 0, 3.3]], 3),
    ([[9.4, 262, 8.5, 3.84, 4, 22, 3.3, 3, 0, 3.3]], 3),
    ([[9.6, 253, 8.4, 3.94, 5, 23, 3.2, 3, 0, 3.1]], 3),
    ([[9.7, 295, 7.6, 4.0, 4, 21, 3.3, 3, 0, 3.4]], 3),
    ([[9.2, 279, 8.0, 3.96, 4, 24, 3.4, 3, 0, 3.2]], 3),
    ([[9.1, 256, 7.7, 3.89, 4, 22, 3.5, 3, 0, 3.1]], 3),
    ([[9.6, 279, 7.7, 4.05, 4, 24, 3.0, 3, 0, 3.4]], 3),
    ([[9.0, 287, 7.7, 3.87, 5, 23, 3.3, 3, 0, 3.2]], 3),
    ([[9.6, 265, 7.7, 4.08, 5, 24, 3.3, 3, 0, 3.5]], 3),
    ([[9.0, 264, 8.4, 3.84, 4, 22, 3.3, 3, 0, 3.3]], 3),
    ([[9.2, 271, 8.3, 3.96, 5, 21, 3.1, 3, 0, 3.2]], 3),
    ([[9.1, 250, 8.4, 3.86, 4, 21, 3.0, 3, 0, 3.1]], 3),
    ([[9.3, 260, 7.1, 3.97, 5, 22, 3.3, 3, 0, 3.1]], 3),
]


def relu(t):
    return np.maximum(t, 0)


def relu_deriv(t):
    return (t >= 0).astype(float)


def softmax(t):
    out = np.exp(t)
    return out / np.sum(out)


def softmax_batch(t):
    out = np.exp(t)
    return out / np.sum(out, axis=1, keepdims=True)


def sparse_cross_entropy_batch(z, y):
    return -np.log(np.array([z[j, y[j]] for j in range(len(y))]))


def to_full_batch(y, num_classes):
    y_full = np.zeros((len(y), num_classes))
    for j, yj in enumerate(y):
        y_full[j, yj] = 1
    return y_full


def predict(x):
    t1 = x @ W1 + b1
    h1 = relu(t1)
    t2 = h1 @ W2 + b2
    z = softmax(t2)

    return z


def calc_accuracy():
    correct = 0
    for x, y in dataset:
        z = predict(x)
        y_pred = np.argmax(z)
        if y_pred == y:
            correct += 1
    acc = correct / len(dataset)
    return acc


input_layer = 10
output_layer = 4
first_layer = 100

W1 = np.random.rand(input_layer, first_layer)
b1 = np.random.rand(1, first_layer)
W2 = np.random.rand(first_layer, output_layer)
b2 = np.random.rand(1, output_layer)

W1 = (W1 - 0.5) * 2 * np.sqrt(1/input_layer)
b1 = (b1 - 0.5) * 2 * np.sqrt(1/input_layer)
W2 = (W2 - 0.5) * 2 * np.sqrt(1/first_layer)
b2 = (b2 - 0.5) * 2 * np.sqrt(1/first_layer)

ALPHA = 0.00001
NUM_EPOCHS = 1000
BATCH_SIZE = 50

loss_arr = []

for ep in range(NUM_EPOCHS):
    random.shuffle(dataset)
    for i in range(len(dataset) // BATCH_SIZE):

        batch_x, batch_y = zip(*dataset[i*BATCH_SIZE: i*BATCH_SIZE+BATCH_SIZE])
        x = np.concatenate(batch_x, axis=0)
        y = np.array(batch_y)

        # Forward

        t1 = x @ W1 + b1
        h1 = relu(t1)
        t2 = h1 @ W2 + b2
        z = softmax_batch(t2)

        E = np.sum(sparse_cross_entropy_batch(z, y))
        loss_arr.append(E)

        # Backward

        y_full = to_full_batch(y, output_layer)
        dE_dt2 = z - y_full
        dE_dW2 = h1.T @ dE_dt2
        dE_db2 = np.sum(dE_dt2, axis=0, keepdims=True)
        dE_dh1 = dE_dt2 @ W2.T
        dE_dt1 = dE_dh1 * relu_deriv(t1)
        dE_dW1 = x.T @ dE_dt1
        dE_db1 = np.sum(dE_dt1, axis=0, keepdims=True)

        # Update

        W1 = W1 - ALPHA * dE_dW1
        b1 = b1 - ALPHA * dE_db1
        W2 = W2 - ALPHA * dE_dW2
        b2 = b2 - ALPHA * dE_db2

accuracy = calc_accuracy()
print("Точность:", accuracy)

plt.plot(loss_arr)
plt.show()

while True:
    print("-" * 200)
    a1, a2, a3, a4, a5, a6, a7, a8, a9, a10 = map(float, input("Введите характеристики: ").split())
    x = np.array([a1, a2, a3, a4, a5, a6, a7, a8, a9, a10])
    probs = predict(x)
    pred_class = np.argmax(probs)
    class_names = ['Ambiance', 'Cara Cara', 'Hamlin', 'Blood Orange']
    print('Полученный класс:', class_names[pred_class])